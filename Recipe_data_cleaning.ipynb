{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (4.46.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from transformers import RobertaTokenizer, RobertaModel, get_scheduler\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "recipe_link = \"https://raw.githubusercontent.com/jade-y-liang/pic16b-project/refs/heads/main/Recipes.csv\"\n",
    "recipes = pd.read_csv(recipe_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_name</th>\n",
       "      <th>category_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>prep_time</th>\n",
       "      <th>cook_time</th>\n",
       "      <th>total_time</th>\n",
       "      <th>num_servings_per_recipe</th>\n",
       "      <th>ingredients_list</th>\n",
       "      <th>direction_list</th>\n",
       "      <th>calories_per_serving</th>\n",
       "      <th>...</th>\n",
       "      <th>protein (g)</th>\n",
       "      <th>carbs (g)</th>\n",
       "      <th>fiber (g)</th>\n",
       "      <th>sugar (g)</th>\n",
       "      <th>cholesterol (mg)</th>\n",
       "      <th>vitamin_c (mg)</th>\n",
       "      <th>calcium (mg)</th>\n",
       "      <th>iron (mg)</th>\n",
       "      <th>potassium (mg)</th>\n",
       "      <th>recipe_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Air Fryer Spicy Onion Rings</td>\n",
       "      <td>Air Fryer Recipes\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20 mins</td>\n",
       "      <td>10 mins</td>\n",
       "      <td>1 hr</td>\n",
       "      <td>4</td>\n",
       "      <td>sweet onions, sliced 1/2 inch thick,buttermilk...</td>\n",
       "      <td>Whisk together buttermilk, egg, flour, chile ...</td>\n",
       "      <td>230.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10g</td>\n",
       "      <td>53g</td>\n",
       "      <td>2g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48mg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>197mg</td>\n",
       "      <td>https://www.allrecipes.com/recipe/8465728/air-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stuffed Chicken Cordon Bleu</td>\n",
       "      <td>Chicken Cordon Bleu\\n</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20 mins</td>\n",
       "      <td>40 mins</td>\n",
       "      <td>1 hr 15 mins</td>\n",
       "      <td>4</td>\n",
       "      <td>skinless, boneless chicken breast halves,bacon...</td>\n",
       "      <td>Preheat the oven to 400 degrees F (200 degree...</td>\n",
       "      <td>877.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66g</td>\n",
       "      <td>44g</td>\n",
       "      <td>3g</td>\n",
       "      <td>8g</td>\n",
       "      <td>291mg</td>\n",
       "      <td>3mg</td>\n",
       "      <td>636mg</td>\n",
       "      <td>4mg</td>\n",
       "      <td>969mg</td>\n",
       "      <td>https://www.allrecipes.com/recipe/283793/stuff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hearty Chicken Cacciatore Soup with Rice</td>\n",
       "      <td>Chicken Cacciatore\\n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10 mins</td>\n",
       "      <td>1 hr 45 mins</td>\n",
       "      <td>1 hr 55 mins</td>\n",
       "      <td>10</td>\n",
       "      <td>chicken broth,condensed tomato soup,water,dice...</td>\n",
       "      <td>Combine chicken broth, condensed soup, 2 cans...</td>\n",
       "      <td>250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15g</td>\n",
       "      <td>35g</td>\n",
       "      <td>2g</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38mg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>406mg</td>\n",
       "      <td>https://www.allrecipes.com/recipe/8300735/hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicken and Dumplings with Biscuits</td>\n",
       "      <td>Chicken and Dumplings\\n</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>whole chicken, cut into pieces,salt,freshly gr...</td>\n",
       "      <td>Put chicken pieces in a large pot over medium...</td>\n",
       "      <td>696.0</td>\n",
       "      <td>...</td>\n",
       "      <td>37g</td>\n",
       "      <td>86g</td>\n",
       "      <td>6g</td>\n",
       "      <td>5g</td>\n",
       "      <td>106mg</td>\n",
       "      <td>49mg</td>\n",
       "      <td>64mg</td>\n",
       "      <td>5mg</td>\n",
       "      <td>1398mg</td>\n",
       "      <td>https://www.allrecipes.com/recipe/8810/chicken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Margo's Chicken Adobo</td>\n",
       "      <td>Chicken Adobo\\n</td>\n",
       "      <td>4.5</td>\n",
       "      <td>10 mins</td>\n",
       "      <td>1 hr</td>\n",
       "      <td>1 hr 10 mins</td>\n",
       "      <td>8</td>\n",
       "      <td>canola oil,chicken drumsticks and thighs,onion...</td>\n",
       "      <td>Heat canola oil in a large Dutch oven over me...</td>\n",
       "      <td>323.0</td>\n",
       "      <td>...</td>\n",
       "      <td>30g</td>\n",
       "      <td>7g</td>\n",
       "      <td>1g</td>\n",
       "      <td>2g</td>\n",
       "      <td>96mg</td>\n",
       "      <td>3mg</td>\n",
       "      <td>40mg</td>\n",
       "      <td>3mg</td>\n",
       "      <td>347mg</td>\n",
       "      <td>https://www.allrecipes.com/recipe/218510/margo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                recipe_name             category_name  rating  \\\n",
       "0               Air Fryer Spicy Onion Rings       Air Fryer Recipes\\n     5.0   \n",
       "1               Stuffed Chicken Cordon Bleu     Chicken Cordon Bleu\\n     5.0   \n",
       "2  Hearty Chicken Cacciatore Soup with Rice      Chicken Cacciatore\\n     NaN   \n",
       "3       Chicken and Dumplings with Biscuits   Chicken and Dumplings\\n     4.1   \n",
       "4                     Margo's Chicken Adobo           Chicken Adobo\\n     4.5   \n",
       "\n",
       "  prep_time      cook_time     total_time num_servings_per_recipe  \\\n",
       "0   20 mins        10 mins           1 hr                      4    \n",
       "1   20 mins        40 mins   1 hr 15 mins                      4    \n",
       "2   10 mins   1 hr 45 mins   1 hr 55 mins                     10    \n",
       "3       NaN            NaN            NaN                      8    \n",
       "4   10 mins           1 hr   1 hr 10 mins                      8    \n",
       "\n",
       "                                    ingredients_list  \\\n",
       "0  sweet onions, sliced 1/2 inch thick,buttermilk...   \n",
       "1  skinless, boneless chicken breast halves,bacon...   \n",
       "2  chicken broth,condensed tomato soup,water,dice...   \n",
       "3  whole chicken, cut into pieces,salt,freshly gr...   \n",
       "4  canola oil,chicken drumsticks and thighs,onion...   \n",
       "\n",
       "                                      direction_list  calories_per_serving  \\\n",
       "0   Whisk together buttermilk, egg, flour, chile ...                 230.0   \n",
       "1   Preheat the oven to 400 degrees F (200 degree...                 877.0   \n",
       "2   Combine chicken broth, condensed soup, 2 cans...                 250.0   \n",
       "3   Put chicken pieces in a large pot over medium...                 696.0   \n",
       "4   Heat canola oil in a large Dutch oven over me...                 323.0   \n",
       "\n",
       "   ... protein (g) carbs (g) fiber (g) sugar (g) cholesterol (mg)  \\\n",
       "0  ...         10g       53g        2g       NaN             48mg   \n",
       "1  ...         66g       44g        3g        8g            291mg   \n",
       "2  ...         15g       35g        2g       NaN             38mg   \n",
       "3  ...         37g       86g        6g        5g            106mg   \n",
       "4  ...         30g        7g        1g        2g             96mg   \n",
       "\n",
       "  vitamin_c (mg) calcium (mg) iron (mg) potassium (mg)  \\\n",
       "0            NaN          NaN       NaN          197mg   \n",
       "1            3mg        636mg       4mg          969mg   \n",
       "2            NaN          NaN       NaN          406mg   \n",
       "3           49mg         64mg       5mg         1398mg   \n",
       "4            3mg         40mg       3mg          347mg   \n",
       "\n",
       "                                         recipe_link  \n",
       "0  https://www.allrecipes.com/recipe/8465728/air-...  \n",
       "1  https://www.allrecipes.com/recipe/283793/stuff...  \n",
       "2  https://www.allrecipes.com/recipe/8300735/hear...  \n",
       "3  https://www.allrecipes.com/recipe/8810/chicken...  \n",
       "4  https://www.allrecipes.com/recipe/218510/margo...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the dataset is loaded correctly\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15198, 23)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recipe_name                 0.00\n",
       "category_name               0.00\n",
       "rating                     11.05\n",
       "prep_time                   7.69\n",
       "cook_time                  20.16\n",
       "total_time                  7.18\n",
       "num_servings_per_recipe     4.78\n",
       "ingredients_list            4.70\n",
       "direction_list              4.72\n",
       "calories_per_serving        5.96\n",
       "total_fat (g)               7.05\n",
       "saturated_fat (g)           9.88\n",
       "sodium (mg)                 5.98\n",
       "protein (g)                 6.28\n",
       "carbs (g)                   6.05\n",
       "fiber (g)                   8.47\n",
       "sugar (g)                  11.51\n",
       "cholesterol (mg)           19.50\n",
       "vitamin_c (mg)             17.81\n",
       "calcium (mg)                8.65\n",
       "iron (mg)                   9.59\n",
       "potassium (mg)              6.00\n",
       "recipe_link                 0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "round(recipes.isna().sum() / len(recipes) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'ingredients_list' and 'direction_list' columns are NaN\n",
    "recipes= recipes.dropna(subset=['ingredients_list', 'direction_list'], how = 'any').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "recipes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Category Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove newline characters from all strings in a specific column\n",
    "recipes['category_name'] = recipes['category_name'].str.replace('\\n', '', regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Direction Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['direction_list'] = recipes['direction_list'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the replacement of \\\\n across the direction_list column\n",
    "recipes['direction_list'] = recipes['direction_list'].str.replace(\"\\n,\", '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify Nutrition Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_cols = [ 'total_fat (g)', 'saturated_fat (g)', 'sodium (mg)', 'protein (g)', \n",
    "                  'carbs (g)', 'fiber (g)', 'sugar (g)', 'cholesterol (mg)', \n",
    "                  'vitamin_c (mg)', 'calcium (mg)', 'iron (mg)', 'potassium (mg)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values \n",
    "# Fill NaN values in the specified columns with 0\n",
    "recipes[nutrition_cols] = recipes[nutrition_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove units and convert to float\n",
    "def remove_units(value):\n",
    "    if isinstance(value, str):\n",
    "        # Extract numeric part using regex\n",
    "        numeric_part = re.search(r'[\\d.]+', value)\n",
    "        if numeric_part:\n",
    "            return float(numeric_part.group(0)) \n",
    "    return float('nan') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each relevant column\n",
    "for col in nutrition_cols:\n",
    "    recipes[col] = recipes[col].apply(remove_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Main Ingredients from Ingredients List by NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand the structure of ingredients list\n",
    "# Stop words to ignore\n",
    "stop_words = {'and', 'or', 'with', 'in', 'on', 'to', 'of', 'for', 'as', 'the', 'a', 'an'}\n",
    "\n",
    "# Function to count all word occurrences\n",
    "def count_word_occurrences(ingredient_strings):\n",
    "    all_words = []\n",
    "    for ingredient_string in ingredient_strings:\n",
    "        if isinstance(ingredient_string, str):\n",
    "            # Normalize text\n",
    "            # Remove punctuation\n",
    "            ingredient_string = re.sub(r'[^\\w\\s]', '', ingredient_string.lower()) \n",
    "            # Tokenize words\n",
    "            words = ingredient_string.split() \n",
    "            # Remove stop words\n",
    "            filtered_words = [word for word in words if word not in stop_words] \n",
    "            all_words.extend(filtered_words) \n",
    "    # Count frequencies\n",
    "    return Counter(all_words)\n",
    "\n",
    "# Count word occurrences for all recipes\n",
    "ingredients_counts = count_word_occurrences(recipes['ingredients_list'])\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "ingredients_counts = (\n",
    "    pd.DataFrame(ingredients_counts.items(), columns=['Word', 'Count'])\n",
    "    .sort_values(by='Count', ascending=False)  # Sort the DataFrame by 'Count'\n",
    "    .reset_index(drop=True)  # Reset the index and drop the old one\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pepper</td>\n",
       "      <td>5230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>black</td>\n",
       "      <td>4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ground</td>\n",
       "      <td>2741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh</td>\n",
       "      <td>2672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cheese</td>\n",
       "      <td>2627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>into</td>\n",
       "      <td>2625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cut</td>\n",
       "      <td>2398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>taste</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>more</td>\n",
       "      <td>1796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>peeled</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word  Count\n",
       "0  pepper   5230\n",
       "1   black   4190\n",
       "2  ground   2741\n",
       "3   fresh   2672\n",
       "4  cheese   2627\n",
       "5    into   2625\n",
       "6     cut   2398\n",
       "7   taste   1972\n",
       "8    more   1796\n",
       "9  peeled   1758"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_counts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords (ingredients to include) and stop words (to exclude)\n",
    "keywords = ['chicken', 'onion', 'tomatoes', 'bread', 'bean', 'bell pepper', 'red pepper', 'sweet pepper' \n",
    "            'lemon', 'beef', 'pork', 'mushroom', 'rice', 'basil', 'cilantro',\n",
    "            'egg', 'milk', 'spinach', 'shrimp', 'orange', 'lime', 'sausage', 'bacon', 'pineapple',\n",
    "            'peanut', 'strawberr', 'coconut', 'pecan', 'apple', 'potato', 'squash', \n",
    "            'jalapeno', 'lettuce', 'tortilla', 'pea', 'amaranth', 'apricot', 'avocado',\n",
    "            'banana', 'barley', 'brisket', 'wheat', 'duck', 'fish', 'flax seed', 'goat',\n",
    "            'turkey', 'lamb', 'mango', 'oat', 'peach', 'pear', 'plum', 'pomegranate', \n",
    "            'salmon', 'shrimp', 'lobster', 'sardine', 'catfish', 'tuna', 'eel ', \n",
    "            'anchovy', 'cucumber', 'eggplant', 'kale', 'lemongrass', 'leek', 'radish', \n",
    "            'cauliflower', 'cabbage', 'asparagus', 'broccoli', 'endive', 'okra', 'sweet potato',\n",
    "            'brussels sprouts', 'leek', 'carrot', 'green beans', 'beet', 'bok choy',\n",
    "            'spinach', 'pumpkin', 'cranberr', 'parsnip', 'grape', 'grapefruit', 'turnip', 'honeydew melon',\n",
    "            'rhubarb', 'blackberr', 'cantaloupe', 'cherr', 'kiwi', 'plum', 'zucchini', \n",
    "            'corn', 'cheese', 'chocolate', 'flour']\n",
    "# keywords = ['chicken', 'onion', 'tomato', 'bread', 'bean', 'bell pepper',\n",
    "#             'lemon', 'beef', 'pork', 'mushroom', 'rice', 'basil', 'cilantro',\n",
    "#             'egg', 'milk', 'spinach', 'shrimp', 'orange', 'lime', 'sausage', 'bacon', 'pineapple',\n",
    "#             'peanut', 'strawberr', 'coconut', 'pecan', 'apple', 'potato', 'squash', \n",
    "#             'jalapeno', 'lettuce', 'tortilla', 'pea', 'amaranth', 'apricot', 'avocado',\n",
    "#             'banana', 'barley', 'brisket', 'wheat', 'bulgur', 'cherr', 'chia seed', 'duck',\n",
    "#             'fish', 'flax seed', 'goat', 'turkey', 'lamb', 'mango', 'millet', 'nectarine',\n",
    "#             'oat', 'peach', 'pear', 'plum', 'pomegranate', 'quinoa', 'seafood', 'shellfish',\n",
    "#             'shell', 'sirloin', 'spelt', 'veal', 'venison', 'octopus', 'sturgeon', \n",
    "#             'squid', 'salmon', 'oyster', 'shrimp', 'lobster', 'caviar', 'carp', 'flounder',\n",
    "#             'crab', 'sardine', 'catfish', 'tuna', 'eel ', 'anchovy', 'collard', 'coriander',\n",
    "#             'cucumber', 'eggplant', 'endive', 'escarole', 'daikon', 'delicata', 'eddoe', 'fennel',\n",
    "#             'kale', 'jicama', 'ginger', 'lemongrass', 'leek', 'habanero', 'radish', 'parsnip',\n",
    "#             'artichoke', 'cauliflower', 'cabbage', 'asparagus', 'kohlrabi',\n",
    "#             'broccoli', 'fiddlehead', 'melon', 'endive', 'okra', 'sweet potato',\n",
    "#             'brussels sprouts', 'leek', 'carrot', 'green beans', 'beet', 'bok choy',\n",
    "#             'spinach', 'pumpkin', 'cranberr', 'parsnip', 'grape', 'grapefruit', 'turnip', 'honeydew melon',\n",
    "#             'rhubarb', 'blackberr', 'cantaloupe', 'cherr', 'kiwi', 'plum', 'peppers', 'zucchini', 'shallot',\n",
    "#             ]\n",
    "# stop_words = ['pepper', 'black', 'ground', 'fresh', 'into', 'cut', 'taste', 'more', 'peeled', 'such', \n",
    "#               'chopped', 'white', 'oil', 'green', 'brown', 'purpose', 'sugar', 'sliced',\n",
    "#               'parmesan', 'red', 'cheddar', 'finely', 'baking', 'sauce', 'optional',\n",
    "#               'thinly', 'salt', 'needed', 'boneless', 'cooking', 'drained', 'seeded',\n",
    "#               'extract', 'cooked', 'room', 'powder', 'juice', 'mozzarella', 'cored',\n",
    "#               'freshly', 'chile', 'at', 'diced', 'grated', 'unsalted', 'rinse', 'inch',\n",
    "#               'plus', 'extract', 'degrees', 'leaves', 'halve', 'whip', 'italian',\n",
    "#               'dried', 'less', 'shred', 'divide', 'sweet', 'whole', 'condense', 'thaw',\n",
    "#               'pitted', 'jack', 'chop', 'lightly', 'frozen', 'minced', 'garnish', 'melted',\n",
    "#               'coarsely', 'mix', 'small', '1', 'yellow', 'flakes', 'frying', 'pack', 'soup',\n",
    "#               'hot', 'light', 'thin', 'crush', 'spray', 'topping', 'squeeze', 'dill', 'sharpe',\n",
    "#               'thyme', 'steak', 'trim', 'large', 'crack', 'food', 'bouillon', 'size', 'sour', 'breast',\n",
    "#               'clove', 'beat']\n",
    "# maybe = ['pepper', 'butter', 'olive', 'cream', 'wine', 'cinnamon', 'cocoa', 'seeds', 'ginger', 'garlic',\n",
    "#          'soy', 'vanilla', 'walnut', 'cider', 'broth', 'salad', 'lean', 'olive', 'pastry', 'sesame', 'vinegar',\n",
    "#          'mustard', 'cream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords from ingredients_list\n",
    "def extract_keywords(ingredient_list, keywords):\n",
    "    # Check if ingredient_list is a string; if not, treat it as empty\n",
    "    if not isinstance(ingredient_list, str):\n",
    "        return ''\n",
    "    \n",
    "    # Normalize text and check for keywords\n",
    "    ingredient_list = ingredient_list.lower()\n",
    "    main_ingredients = [keyword for keyword in keywords if keyword in ingredient_list]\n",
    "    \n",
    "    return sorted(set(main_ingredients))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to DataFrame\n",
    "recipes['main_ingredients'] = recipes['ingredients_list'].apply(lambda x: extract_keywords(x, keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'err' with 'erries' in one line\n",
    "recipes['main_ingredients'] = recipes['main_ingredients'].apply(lambda ingredients: [ingredient.replace('err', 'erries') for ingredient in ingredients])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bread', 'egg', 'flour', 'lime', 'milk', 'onion']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes['main_ingredients'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cols = ['prep_time', 'cook_time', 'total_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert time string to total minutes\n",
    "def convert_to_minutes(time_str):\n",
    "    # Handle NaN or non-string values\n",
    "    if not isinstance(time_str, str):\n",
    "        return np.nan \n",
    "    \n",
    "    # Initialize hours and minutes\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    \n",
    "    # Extract hours and minutes using regex\n",
    "    if 'hr' in time_str:\n",
    "        hours_match = re.search(r'(\\d+)\\s*hr', time_str)\n",
    "        if hours_match:\n",
    "            hours = int(hours_match.group(1))\n",
    "    \n",
    "    if 'min' in time_str:\n",
    "        minutes_match = re.search(r'(\\d+)\\s*min', time_str)\n",
    "        if minutes_match:\n",
    "            minutes = int(minutes_match.group(1))\n",
    "    \n",
    "    # Convert total time to minutes\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    return int(total_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each time column\n",
    "for col in time_cols:\n",
    "    recipes[col] = recipes[col].apply(convert_to_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the additional time column\n",
    "recipes['additional_time'] = recipes['total_time'] - recipes['cook_time'] - recipes['prep_time']\n",
    "\n",
    "# Reorder the columns to place 'additional_time' before 'total_time'\n",
    "columns = recipes.columns.tolist()\n",
    "columns.insert(columns.index('total_time'), columns.pop(columns.index('additional_time')))\n",
    "recipes = recipes[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Empty Time Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows with NaN in 'total_time' column\n",
    "nan_time = recipes[recipes['total_time'].isna()].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Put chicken pieces in a large pot over medium heat, and add enough water to cover 3 inches over the chicken. Add the salt, celery, pepper, carrots, celery and potatoes. Bring to a boil and let simmer for 40 minutes.\n",
      " In a mixing bowl, prepare biscuit mix according to package directions. After simmering soup for 40 minutes, drop rounded tablespoonfuls of the biscuit mixture into the pot and let them cook a bit between additions of dumplings, so that they do not stick together.\n",
      " With a wire whisk, mix together about 1 1/2 cups of water with the flour, then add flour mixture to the soup pot until the broth has the consistency of gravy. Dig in and enjoy the meal!\n",
      "\n",
      " Whisk together flour, baking soda, salt, cinnamon, and baking powder.\n",
      " In a large bowl, beat the eggs. Gradually beat in sugar, then oil. Add flour mixture alternately with zucchini into the egg mixture. Stir in the raisins, walnuts, and vanilla. Pour batter into two 9 x 5 inch greased and lightly floured loaf pans.\n",
      " Bake on lowest rack of the oven at 350 degrees F (175 degrees C) for 55 minutes. Let cool for 10 minutes in the pan, then turn out onto racks to cool completely. To freeze, wrap loaves in plastic wrap, and then wrap in heavy freezer paper. Will keep indefinitely.\n",
      "\n",
      " Saute mushrooms in 1 tablespoon of oil. Season with salt and set aside.\n",
      " Whisk half-and-half, eggs, salt and pepper until smooth. Spray a 9-by-13-inch Pyrex or ceramic baking dish with vegetable cooking spray.\n",
      " Line bottom with 6 slices of bread. Scatter half of the ham, and half of the mushrooms over the bread, then sprinkle with half of the scallions and cheese. Pour 1 cup of egg mixture over the top. Repeat layers with remaining bread, ham, mushrooms, cheese and scallions. Slowly pour remaining egg mixture evenly over top. Cover with plastic wrap, then weight down casserole with 3 16-ounce cans for at least 15 minutes to submerge ingredients. (Can be refrigerated overnight, but return to room temperature before baking.)\n",
      " Adjust oven rack to middle position. Bake in preheated 325-degree oven until custard is just set, about 50 minutes. Turn on broiler and broil until strata is spotty brown and puffy (watch carefully), about 5 minutes longer. Let stand for 8 to 10 minutes, then serve immediately.\n",
      "\n",
      " Pre-heat oven to 350 degrees F (175 degrees C). Lightly grease a 9 x 9 x 1-3/4 inch pan.\n",
      " Sift together flour, baking powder and salt and set aside. In large bowl, beat together butter, sugar, egg and vanilla until smooth.\n",
      " Stir in the flour mixture and the nuts until well blended. Spread evenly in prepared pan. Bake 25-30 minutes or until surface springs back when gently pressed. Cool slightly. While still warm, cut into bars with a sharp knife.\n",
      "\n",
      " Preheat oven to 325 degrees F (165 degrees C).\n",
      " Combine brownie mix, oil, egg and water in a large bowl. Stir 50 strokes with a spoon.\n",
      " Place batter in a cupcake pan (cupcake wrappers recommended). Once cups are 3/4 of the way full, place an unwrapped miniature peanut butter cup in the middle, then bake for 30-35 minutes keeping an eye on them while they're baking. Let them cool and then they're ready to eat!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display directions for reference\n",
    "for i in range(5):\n",
    "    print(nan_time.iloc[i, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to normalize target \n",
    "def normalize_target(targets):\n",
    "    scaler = MinMaxScaler() \n",
    "    normalized_targets = scaler.fit_transform(targets.reshape(-1, 1))\n",
    "    return normalized_targets, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a cleaned dataset without NaN values inside total_time\n",
    "recipes_clean = recipes.dropna(subset=['total_time']).reset_index(drop=True)\n",
    "recipes_clean['processed_directions'] = recipes_clean['direction_list'].apply(preprocess_text)\n",
    "normalized_targets, target_scaler = normalize_target(recipes_clean['total_time'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data with a function\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding = \"max_length\",\n",
    "        truncation = True,\n",
    "        max_length = 500,\n",
    "        return_tensors = \"pt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the processed text using the transformer tokenizer\n",
    "encoded_inputs = tokenizer(\n",
    "    recipes_clean['processed_directions'].tolist(), \n",
    "    padding = True, \n",
    "    truncation = True, \n",
    "    return_tensors = 'pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_targets, val_targets = train_test_split(\n",
    "    encoded_inputs['input_ids'],\n",
    "    normalized_targets,\n",
    "    test_size = 0.2,\n",
    "    random_state = 123\n",
    ")\n",
    "\n",
    "train_texts = train_texts.tolist()\n",
    "val_texts = val_texts.tolist()\n",
    "\n",
    "# # Reset the index for train and validation targets\n",
    "# train_targets = train_targets.reset_index(drop=True)\n",
    "# val_targets = val_targets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Training & Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset class\n",
    "class RecipeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, targets):\n",
    "        self.encodings = encodings\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_encodings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m RecipeDataset(\u001b[43mtrain_encodings\u001b[49m, train_targets)\n\u001b[1;32m      2\u001b[0m val_df \u001b[38;5;241m=\u001b[39m RecipeDataset(val_encodings, val_targets)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_encodings' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = RecipeDataset(train_encodings, train_targets)\n",
    "val_df = RecipeDataset(val_encodings, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_df, batch_size = 8, shuffle = True)\n",
    "val_loader = DataLoader(val_df, batch_size = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define RoBERTa Model with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaForRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaForRegression, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.regressor = nn.Linear(self.roberta.config.hidden_size, 1)  # Output: single value for regression\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # CLS token output\n",
    "        return self.regressor(pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Model\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = RobertaForRegression().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|▎         | 52/1411 [08:43<3:47:55, 10.06s/batch, loss=3.98e+4, lr=0.000491]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39msqueeze(), targets)  \n\u001b[1;32m     15\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36mRobertaForRegression.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output  \u001b[38;5;66;03m# CLS token output\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(pooled_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    622\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m         output_attentions,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    510\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:447\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    439\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    446\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 447\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    457\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/PIC16B-24F/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:370\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    368\u001b[0m )\n\u001b[0;32m--> 370\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    380\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()  \n",
    "    train_loss = 0  \n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\") as pbar:\n",
    "        for step, batch in enumerate(pbar):\n",
    "            optimizer.zero_grad()  \n",
    "            input_ids = batch[\"input_ids\"].to(device) \n",
    "            attention_mask = batch[\"attention_mask\"].to(device) \n",
    "            targets = batch[\"labels\"].to(device) \n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = model(input_ids, attention_mask)  \n",
    "            loss = loss_fn(outputs.squeeze(), targets)  \n",
    "            train_loss += loss.item()\n",
    "            # Backpropagation\n",
    "            loss.backward()  # Backpropagation\n",
    "            # Update the model parameters\n",
    "            optimizer.step()  \n",
    "            # Adjust the learning rate\n",
    "            scheduler.step()  \n",
    "\n",
    "            current_lr = scheduler.get_last_lr()[0] \n",
    "            pbar.set_postfix(loss=train_loss / (step + 1), lr=current_lr)  \n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)  \n",
    "    print(f\"Epoch {epoch + 1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "    current_lr = scheduler.get_last_lr()[0] \n",
    "    print(f\"Epoch {epoch + 1}, Final Learning Rate: {current_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PIC16B-24F",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
